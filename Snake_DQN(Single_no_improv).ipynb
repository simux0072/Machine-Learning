{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Snake DQN(Single_no_improv).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNsKNB8p9f5yk9StKKsl8q6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simux0072/Machine-Learning/blob/master/Snake_DQN(Single_no_improv).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "import IPython"
      ],
      "metadata": {
        "id": "xrEnniUlinS8"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Snake game"
      ],
      "metadata": {
        "id": "o22E06SIVFxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "uYbkMel4tQyl"
      },
      "outputs": [],
      "source": [
        "DIMENSIONS = (16, 12)\n",
        "SIZE = 30\n",
        "\n",
        "DIRECTIONS = {\n",
        "    0: (0, -1), # UP\n",
        "    1: (1, 0),  # RIGHT\n",
        "    2: (0, 1),  # DOWN\n",
        "    3: (-1, 0)  # LEFT \n",
        "}\n",
        "\n",
        "class player():\n",
        "    def __init__(self):\n",
        "        self.size = SIZE\n",
        "        self.color = {\n",
        "            'Outer': (0, 0, 255),\n",
        "            'Inner': (0, 100, 255)\n",
        "        }\n",
        "        self.points = 0\n",
        "        self.head = {\n",
        "            'Name': 'Head',\n",
        "            'Coordinates': [round(DIMENSIONS[0]/2), round(DIMENSIONS[1]/2)],\n",
        "            'Direction': 3\n",
        "        }\n",
        "        self.snake = [self.head]\n",
        "        self.iter = 0\n",
        "    \n",
        "    def _move(self, move):\n",
        "        for i in range(0, len(self.snake)):\n",
        "            if i == len(self.snake) - 1:\n",
        "                self.snake[i]['Direction'] += move - 1      # Change direction\n",
        "                if self.snake[i]['Direction'] >= 4:     # Check if need to change the direction number\n",
        "                    self.snake[i]['Direction'] = 0\n",
        "                elif self.snake[i]['Direction'] < 0:\n",
        "                    self.snake[i]['Direction'] = 3\n",
        "            else:\n",
        "                self.snake[i]['Direction'] = self.snake[i + 1]['Direction']\n",
        "                \n",
        "            self.iter += 1  # Add 1 to iter counter\n",
        "            self.snake[i]['Coordinates'][0] += DIRECTIONS[self.snake[i]['Direction']][0] # Change the coordinates of the snake\n",
        "            self.snake[i]['Coordinates'][1] += DIRECTIONS[self.snake[i]['Direction']][1]\n",
        "\n",
        "    def _collision(self):\n",
        "        if self.snake[-1]['Coordinates'][0] < 0 or self.snake[-1]['Coordinates'][0] > DIMENSIONS[0] - 1:\n",
        "            return True, -1\n",
        "        elif self.snake[-1]['Coordinates'][1] < 0 or self.snake[-1]['Coordinates'][1] > DIMENSIONS[1] - 1:\n",
        "            return True, -1\n",
        "        \n",
        "        for i in self.snake[::-1]:\n",
        "            if i != self.snake[-1] and i['Coordinates'] == self.snake[-1]['Coordinates']:\n",
        "                return True, -1\n",
        "        return False, 0\n",
        "\n",
        "    def _ate_food(self, food_):\n",
        "        if self.snake[-1]['Coordinates'] == food_.coordinates:\n",
        "            temp = {\n",
        "                'Name': 'Body',\n",
        "                'Coordinates': [self.snake[0]['Coordinates'][0] - DIRECTIONS[self.snake[0]['Direction']][0], \n",
        "                            self.snake[0]['Coordinates'][1] - DIRECTIONS[self.snake[0]['Direction']][1]],\n",
        "                'Direction': self.snake[0]['Direction']\n",
        "            }\n",
        "            self.snake.insert(0, temp) # Insert new body\n",
        "            self.iter = 0\n",
        "            self.points += 1\n",
        "            food_._generate()\n",
        "            return True, 1\n",
        "        return False, 0 \n",
        "            \n",
        "\n",
        "    def _too_many_moves(self):\n",
        "        if self.iter > len(self.snake) * 50: # Check if there were too many moves\n",
        "            return True, -50\n",
        "        return False, 0\n",
        "\n",
        "    def _play(self, food_, move):\n",
        "        self._move(move)\n",
        "        # self._draw(food_)\n",
        "        game_end, reward = self._too_many_moves()\n",
        "        if game_end:\n",
        "            return reward, self.points, game_end\n",
        "        \n",
        "        ate_food, reward = self._ate_food(food_)\n",
        "        if ate_food:\n",
        "            return reward, self.points, False\n",
        "        else:\n",
        "            game_end, reward = self._collision()\n",
        "            if game_end:\n",
        "                return reward, self.points, game_end\n",
        "        \n",
        "        return 0, self.points, False\n",
        "\n",
        "class food():\n",
        "    def __init__(self, player_):\n",
        "        self.coordinates = []\n",
        "        self.player = player_\n",
        "        self._generate()\n",
        "        self.color = {\n",
        "            'Red': (200, 0, 0)\n",
        "        }\n",
        "\n",
        "    def _generate(self):\n",
        "        while True:\n",
        "            _is_same = False\n",
        "            self.coordinates = [random.randrange(0, DIMENSIONS[0] - 1),\n",
        "                            random.randrange(0, DIMENSIONS[1] - 1)]\n",
        "            for i in self.player.snake:\n",
        "                if i['Coordinates'] == self.coordinates:\n",
        "                    _is_same = True\n",
        "            \n",
        "            if not _is_same:\n",
        "                break\n",
        "\n",
        "class environment():\n",
        "    def __init__(self, player, food):\n",
        "        self.player_ = player\n",
        "        self.food_ = food\n",
        "        self.state_head = np.zeros((3, DIMENSIONS[1], DIMENSIONS[0]), dtype=np.float32)\n",
        "        self.state_body = np.zeros((3, DIMENSIONS[1], DIMENSIONS[0]), dtype=np.float32)\n",
        "        self.state_food = np.zeros((2, DIMENSIONS[1], DIMENSIONS[0]), dtype=np.float32)\n",
        "\n",
        "    def _get_current_state(self):\n",
        "        temp_state_head = np.zeros((DIMENSIONS[1], DIMENSIONS[0]))\n",
        "        temp_state_body = np.zeros((DIMENSIONS[1], DIMENSIONS[0]))\n",
        "        temp_state_food = np.zeros((DIMENSIONS[1], DIMENSIONS[0]))\n",
        "        for i in self.player_.snake:\n",
        "            if i['Name'] == 'Body':\n",
        "                temp_state_body[i['Coordinates'][1]][i['Coordinates'][0]] = 1\n",
        "            elif i['Name'] == 'Head':\n",
        "                temp_state_head[i['Coordinates'][1]][i['Coordinates'][0]] = 1\n",
        "        temp_state_food[self.food_.coordinates[1]][self.food_.coordinates[0]] = 1\n",
        "\n",
        "        self.state_body = np.insert(np.delete(self.state_body, -1, axis=0), 0, temp_state_body, axis=0)\n",
        "        self.state_head = np.insert(np.delete(self.state_head, -1, axis=0), 0, temp_state_head, axis=0)\n",
        "        self.state_food = np.insert(np.delete(self.state_food, -1, axis=0), 0, temp_state_food, axis=0)\n",
        "        return np.concatenate((self.state_head, self.state_body, self.state_food))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent class"
      ],
      "metadata": {
        "id": "jMfP-ultt-zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, strategy, num_actions, device, target_net, policy_net, lr, gamma, checkpoint):\n",
        "        self.current_step = 0\n",
        "        self.strategy = strategy\n",
        "        self.num_actions = num_actions\n",
        "        self.device = device\n",
        "        self.target_net = target_net\n",
        "        self.policy_net = policy_net\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), self.lr)\n",
        "        if checkpoint is not None:\n",
        "          self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "          self.current_step = checkpoint['current_step']\n",
        "\n",
        "    def select_action(self, state, policy_net):\n",
        "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
        "        self.current_step += 1\n",
        "\n",
        "        if rate > random.random():\n",
        "            action = random.randrange(self.num_actions)\n",
        "            return torch.tensor([action]).to(self.device) #Explore\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return policy_net(state).argmax(dim = 1).to(self.device) #Exploit \n",
        "    \n",
        "    def train_memory(self, states, actions, rewards, next_states, mask):\n",
        "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        next_q_values = self.target_net(next_states).max(1)[0]\n",
        "        discounted_q_values = rewards.squeeze(1) + next_q_values * self.gamma * mask.type(torch.float32)\n",
        "\n",
        "        loss = torch.square(discounted_q_values - current_q_values).mean()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def net_update(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "class EpsilonGreedyStrat():\n",
        "    def __init__(self, start, end, decay):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.decay = decay\n",
        "\n",
        "    def get_exploration_rate(self, current_step):\n",
        "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
      ],
      "metadata": {
        "id": "frCXWXTJtbkR"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN Class"
      ],
      "metadata": {
        "id": "nwG9Ad-iv-gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, num_layer):\n",
        "        super().__init__()\n",
        "        self.conv1_1 = nn.Conv2d(in_channels=num_layer, out_channels=num_layer, kernel_size=3, groups=4, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(in_channels=num_layer, out_channels=num_layer, kernel_size=3, groups=4, padding=1)\n",
        "        self.conv2_1 = nn.Conv2d(in_channels=num_layer, out_channels=num_layer, kernel_size=3, groups=4, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(in_channels=num_layer, out_channels=num_layer, kernel_size=3, groups=4, padding=1)\n",
        "        \n",
        "        self.norm1_1 = nn.BatchNorm2d(num_features=num_layer)\n",
        "        self.norm1_2 = nn.BatchNorm2d(num_features=num_layer)\n",
        "        self.norm2_1 = nn.BatchNorm2d(num_features=num_layer)\n",
        "        self.norm2_2 = nn.BatchNorm2d(num_features=num_layer)\n",
        "\n",
        "        self.conv_res1 = nn.Conv2d(in_channels=num_layer, out_channels=num_layer, kernel_size=1, groups=4)\n",
        "        self.conv_res2 = nn.Conv2d(in_channels=num_layer, out_channels=num_layer, kernel_size=1, groups=4)\n",
        "\n",
        "        self.globpool = nn.AvgPool2d(kernel_size=(6, 10), stride=2, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=8*5*5, out_features=112)\n",
        "        self.fc2 = nn.Linear(in_features=112, out_features=56)\n",
        "        self.fc3 = nn.Linear(in_features=56, out_features=28)\n",
        "        self.fc4 = nn.Linear(in_features=28, out_features=14)\n",
        "        self.out = nn.Linear(in_features=14, out_features=3)\n",
        "\n",
        "    def forward(self, t):\n",
        "        Y = t\n",
        "        t = F.relu(self.norm1_1(self.conv1_1(t)))\n",
        "        t = F.relu(self.norm1_2(self.conv1_2(t)) + self.conv_res1(Y))\n",
        "\n",
        "        Y = t\n",
        "        t = F.relu(self.norm2_1(self.conv2_1(t)))\n",
        "        t = F.relu(self.norm2_2(self.conv2_2(t)) + self.conv_res2(Y))\n",
        "        \n",
        "        t = self.globpool(t)\n",
        "\n",
        "        t = t.flatten(start_dim=1)\n",
        "        t = F.relu(self.fc1(t))\n",
        "        t = F.relu(self.fc2(t))\n",
        "        t = F.relu(self.fc3(t))\n",
        "        t = F.relu(self.fc4(t))\n",
        "        t = F.relu(self.out(t))\n",
        "        return t\n"
      ],
      "metadata": {
        "id": "b0DWZc4ZuQj1"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environmnet manager"
      ],
      "metadata": {
        "id": "YhLS7edwvzfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvManager():\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        self.done = False\n",
        "        self.player = player()\n",
        "        self.food = food(self.player)\n",
        "        self.env = environment(self.player, self.food)\n",
        "\n",
        "    def reset(self):\n",
        "        self.player = player()\n",
        "        self.food = food(self.player)\n",
        "        self.env = environment(self.player, self.food)\n",
        "        self.done = False\n",
        "    \n",
        "    def take_action(self, action):\n",
        "        reward, points, self.done = self.player._play(self.food, action.item())\n",
        "        return torch.tensor([reward], device=self.device), points, torch.tensor([self.done], device=self.device)\n",
        "\n",
        "    def get_state(self):\n",
        "        if self.done:\n",
        "            return torch.zeros((1, 8, DIMENSIONS[1], DIMENSIONS[0]), dtype=torch.float32).to(self.device)\n",
        "        else:\n",
        "            state = torch.from_numpy(self.env._get_current_state()).unsqueeze(dim=0).to(self.device)\n",
        "        return state\n"
      ],
      "metadata": {
        "id": "1i-IemeZu1H6"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Memory"
      ],
      "metadata": {
        "id": "aNYAaVljwSuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "    \n",
        "    def push(self, experience):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(experience)\n",
        "        else:\n",
        "            self.memory.append(experience)\n",
        "            del self.memory[0]\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def can_provide_sample(self, batch_size):\n",
        "        return len(self.memory) >= batch_size * 3"
      ],
      "metadata": {
        "id": "aewXz1phvND8"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data saving and information output to screen\n"
      ],
      "metadata": {
        "id": "pNVyAmmYGZG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class bar_update():\n",
        "  def __init__(self):\n",
        "    self.bar_lenght = 50\n",
        "    self.bar_prog = 0\n",
        "    self.bar = list('[' + ' ' * (self.bar_lenght) + ']')\n",
        "    self.out = display(IPython.display.Pretty(''), display_id=True)\n",
        "\n",
        "  def new_bar(self):\n",
        "    self.bar = list('[' + ' ' * (self.bar_lenght) + ']')\n",
        "    self.bar_prog = 0\n",
        "    self.out = display(IPython.display.Pretty(''), display_id=True)\n",
        "\n",
        "  def print_info(self, num_ep, loss, score, high_schore, update, num_update):\n",
        "    base = int(num_ep / (update / self.bar_lenght)) \n",
        "    if base != self.bar_prog:\n",
        "      self.bar_prog = base\n",
        "      if base == 1:\n",
        "        self.bar[1] = '>'\n",
        "      else:\n",
        "        self.bar[base - 1] = '='\n",
        "        self.bar[base] = '>'\n",
        "    self.out.update(IPython.display.Pretty(''.join(self.bar) + ' Episode: ' + str(num_update) + ' Game: ' + str(num_ep) + ' loss: ' + str(loss) + ' score: ' + str(score) + ' High Score: ' + str(High_score)))"
      ],
      "metadata": {
        "id": "3_1wTQ7kY9K8"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbp38HJeUGCT",
        "outputId": "e562bb6c-95eb-414b-95e6-3f327ddcd0b7"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import exists\n",
        "\n",
        "class model_drive():\n",
        "  def __init__(self, path, model_name):\n",
        "    self.path = path\n",
        "    self.model_name = model_name\n",
        "\n",
        "  def upload(self, model, optimizer, update, current_step):\n",
        "    torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'update': update,\n",
        "            'current_step': current_step\n",
        "            }, self.path + self.model_name)\n",
        "\n",
        "  def does_exist(self):\n",
        "    return exists(self.path + self.model_name)\n",
        "\n",
        "  def download(self):\n",
        "    return torch.load(self.path + self.model_name)"
      ],
      "metadata": {
        "id": "aRU6poXpV2WB"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Class"
      ],
      "metadata": {
        "id": "NILMiKe5wsPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mini_batch = 256\n",
        "gamma = 0.99\n",
        "eps_start = 1\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.00001\n",
        "target_update = 10\n",
        "memory_size = 1000000\n",
        "lr = 0.00001\n",
        "\n",
        "Experience = namedtuple(\n",
        "  'Experience',\n",
        "  ('state', 'action', 'reward', 'next_state', 'end_state')\n",
        ")\n",
        "\n",
        "def extract_tensors(experience):\n",
        "  batch = Experience(*zip(*experience))\n",
        "\n",
        "  t1 = torch.cat(batch.state)\n",
        "  t2 = torch.cat(batch.action)\n",
        "  t3 = torch.cat(batch.reward).unsqueeze(dim=-1)\n",
        "  t4 = torch.cat(batch.next_state)\n",
        "  t5 = torch.cat(batch.end_state)\n",
        "  return (t1, t2, t3, t4, t5)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "path = '/content/drive/MyDrive/models/'\n",
        "model_name = 'DQN_Snake.mdl'\n",
        "\n",
        "bar = bar_update()\n",
        "drive = model_drive(path, model_name)\n",
        "\n",
        "model_exists = drive.does_exist()\n",
        "\n",
        "policy_net = DQN(8).to(device)\n",
        "\n",
        "if model_exists:\n",
        "  checkpoint = drive.download()\n",
        "  update = checkpoint['update']\n",
        "  policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "else:\n",
        "  checkpoint = None\n",
        "  update = 1\n",
        "\n",
        "target_net = DQN(8).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "em = EnvManager(device)\n",
        "strategy = EpsilonGreedyStrat(eps_start, eps_end, eps_decay)\n",
        "agent = Agent(strategy, 3, device, target_net, policy_net, lr, gamma, checkpoint)\n",
        "memory = ReplayMemory(memory_size)\n",
        "\n",
        "\n",
        "points_all = 0\n",
        "loss_all = 0\n",
        "score_all = 0\n",
        "High_score = 0\n",
        "episode = 0\n",
        "\n",
        "\n",
        "while True:\n",
        "  episode += 1\n",
        "  iter = 0\n",
        "  loss = 0\n",
        "  points = 0\n",
        "  state = em.get_state()\n",
        "\n",
        "  for timestep in count():\n",
        "\n",
        "    action = agent.select_action(state, policy_net)\n",
        "    reward, points, end_state = em.take_action(action)\n",
        "    next_state = em.get_state()\n",
        "    memory.push(Experience(state, action, reward, next_state, end_state))\n",
        "    state = next_state\n",
        "\n",
        "    if memory.can_provide_sample(mini_batch):\n",
        "        experience = memory.sample(mini_batch)\n",
        "        states, actions, rewards, next_states, mask = extract_tensors(experience)\n",
        "\n",
        "        loss += agent.train_memory(states, actions, rewards, next_states, mask)\n",
        "        iter += 1\n",
        "\n",
        "    if em.done:\n",
        "        em.reset()\n",
        "        if loss != 0:\n",
        "          loss_all += loss / iter\n",
        "        score_all += points\n",
        "        if points > High_score:\n",
        "            High_score = points\n",
        "        bar.print_info(episode, loss_all / episode, score_all / episode, High_score, target_update, update)\n",
        "        break\n",
        "\n",
        "  if episode % target_update == 0:\n",
        "      agent.net_update()\n",
        "      update += 1\n",
        "\n",
        "      drive.upload(agent.policy_net, agent.optimizer, update, agent.current_step)\n",
        "\n",
        "      bar.new_bar()\n",
        "\n",
        "      episode = 0\n",
        "      loss_all = 0\n",
        "      score_all = 0\n",
        "      High_score = 0"
      ],
      "metadata": {
        "id": "5V_8Y2WGxaLX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}